{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This class is designed for the Borusan Hackathon to efficiently extract and segment text from PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in ./v-env/lib/python3.9/site-packages (0.10.3)\n",
      "Requirement already satisfied: pdfminer.six==20221105 in ./v-env/lib/python3.9/site-packages (from pdfplumber) (20221105)\n",
      "Requirement already satisfied: Pillow>=9.1 in ./v-env/lib/python3.9/site-packages (from pdfplumber) (10.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in ./v-env/lib/python3.9/site-packages (from pdfplumber) (4.25.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in ./v-env/lib/python3.9/site-packages (from pdfminer.six==20221105->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in ./v-env/lib/python3.9/site-packages (from pdfminer.six==20221105->pdfplumber) (41.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in ./v-env/lib/python3.9/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in ./v-env/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber\n",
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "class BorusanHackathonPDFExtractor:\n",
    "    def __init__(self, file_path, chunk_size=500):\n",
    "        \"\"\"\n",
    "        Initializes the BorusanHackathonPDFExtractor.\n",
    "\n",
    "        Args:\n",
    "        file_path (str): The path to the PDF file for the Borusan Hackathon.\n",
    "        chunk_size (int): Maximum size of each text chunk.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.full_text = \"\"\n",
    "        self.chunks = []\n",
    "\n",
    "    def extract_text(self):\n",
    "        \"\"\"Extracts text from the PDF file.\"\"\"\n",
    "        with pdfplumber.open(self.file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                self.full_text += page.extract_text() or \"\"\n",
    "\n",
    "    def split_into_chunks(self):\n",
    "        \"\"\"\n",
    "        Splits the extracted text into chunks of up to chunk_size.\n",
    "        Chunks are split at the last period before the chunk_size limit.\n",
    "        \"\"\"\n",
    "        text = self.full_text\n",
    "        while len(text) > self.chunk_size:\n",
    "            last_period_index = text[:self.chunk_size].rfind('.')\n",
    "            if last_period_index == -1:\n",
    "                last_period_index = self.chunk_size\n",
    "            self.chunks.append(text[:last_period_index].strip())\n",
    "            text = text[last_period_index+1:]\n",
    "\n",
    "        if text:\n",
    "            self.chunks.append(text.strip())\n",
    "\n",
    "    def get_chunks(self):\n",
    "        \"\"\"Returns the list of text chunks.\"\"\"\n",
    "        return self.chunks\n",
    "\n",
    "    def process_pdf(self):\n",
    "        \"\"\"Processes the PDF file to extract and split text.\"\"\"\n",
    "        self.extract_text()\n",
    "        self.split_into_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = BorusanHackathonPDFExtractor(\"source/borusan.pdf\")\n",
    "extractor.process_pdf()\n",
    "chunks = extractor.get_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai python-dotenv\n",
    "!pip install --upgrade qdrant_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### see openai version below (for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.8\n"
     ]
    }
   ],
   "source": [
    "import openai \n",
    "print(openai.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Azure OpenAI client with API credentials loaded from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "RESOURCE_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=API_KEY,\n",
    "  api_version=\"2023-05-15\",\n",
    "  azure_endpoint=RESOURCE_ENDPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration for the embedding model: model name - openai embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"text-embedding-ada-002\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert text chunks to PointStructs with embeddings for Qdrant indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "def create_point_structs_from_chunks(chunks):\n",
    "    \"\"\"\n",
    "    Verilen metin parçalarından (chunks) PointStruct nesneleri oluşturur.\n",
    "    \n",
    "    Args:\n",
    "    chunks (list of str): Metin parçalarının listesi.\n",
    "\n",
    "    Returns:\n",
    "    list of PointStruct: Oluşturulan PointStruct nesnelerinin listesi.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "\n",
    "    for index, chunk in enumerate(chunks, start=1):\n",
    "        embeddings = client.embeddings.create(input=chunk, model=embedding_model).data[0].embedding\n",
    "        points.append(PointStruct(id=index, vector=embeddings, payload={\"text\": chunk}))\n",
    "\n",
    "    print(\"Embedding size:\", len(embeddings)) # VectorParams fonksiyonu için embedding size'ı döndürmemiz gerekiyor. Buna göre size parametresini ayarlayabilirsiniz.\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 1536\n"
     ]
    }
   ],
   "source": [
    "points = create_point_structs_from_chunks(chunks)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Create a client instance for Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create collection with vector configuration - Cosine distance and 1536 vector size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=\"AutoHackathon\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Upsert points to the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(\n",
    "    collection_name=\"AutoHackathon\",\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### see collection info or open http://localhost:6333/collections/AutoHackathon in browser for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in the Coderspace - Auto Hackathon collection: 66\n"
     ]
    }
   ],
   "source": [
    "collection_info = client.get_collection(collection_name=\"AutoHackathon\")\n",
    "print(\"Number of vectors in the Coderspace - Auto Hackathon collection:\", collection_info.vectors_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
